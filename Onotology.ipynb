{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7cce48e-098f-4b7e-ba11-c7782fe0b1bf",
   "metadata": {},
   "source": [
    "# Onotological analysis of EcoSIM variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73ea5b99-edfb-4e54-83d4-af69af77162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository cloned successfully.\n",
      "Processing dir:./EcoSIM_code/f90src/Ecosim_datatype/\n",
      "Copied: SoilPropertyDataType.F90 -> SoilPropertyDataType.txt\n",
      "Copied: FertilizerDataType.F90 -> FertilizerDataType.txt\n",
      "Copied: CanopyRadDataType.F90 -> CanopyRadDataType.txt\n",
      "Copied: SnowDataType.F90 -> SnowDataType.txt\n",
      "Copied: SOMDataType.F90 -> SOMDataType.txt\n",
      "Copied: SoilWaterDataType.F90 -> SoilWaterDataType.txt\n",
      "Copied: AqueChemDatatype.F90 -> AqueChemDatatype.txt\n",
      "Excluded: EcoSIMCtrlDataType.F90\n",
      "Copied: ClimForcDataType.F90 -> ClimForcDataType.txt\n",
      "Copied: SedimentDataType.F90 -> SedimentDataType.txt\n",
      "Excluded: BalanceCheckDataType.F90\n",
      "Copied: SoilHeatDataType.F90 -> SoilHeatDataType.txt\n",
      "Excluded: EcoSIMHistMod.F90\n",
      "Copied: SoilPhysDataType.F90 -> SoilPhysDataType.txt\n",
      "Copied: SurfSoilDataType.F90 -> SurfSoilDataType.txt\n",
      "Copied: LandSurfDataType.F90 -> LandSurfDataType.txt\n",
      "Copied: PlantTraitDataType.F90 -> PlantTraitDataType.txt\n",
      "Copied: SoilBGCDataType.F90 -> SoilBGCDataType.txt\n",
      "Copied: ChemTranspDataType.F90 -> ChemTranspDataType.txt\n",
      "Copied: RootDataType.F90 -> RootDataType.txt\n",
      "Copied: SurfLitterDataType.F90 -> SurfLitterDataType.txt\n",
      "Copied: MicrobialDataType.F90 -> MicrobialDataType.txt\n",
      "Copied: PlantMgmtDataType.F90 -> PlantMgmtDataType.txt\n",
      "Copied: GridDataType.F90 -> GridDataType.txt\n",
      "Copied: EcosimBGCFluxType.F90 -> EcosimBGCFluxType.txt\n",
      "Copied: PlantDataRateType.F90 -> PlantDataRateType.txt\n",
      "Copied: IrrigationDataType.F90 -> IrrigationDataType.txt\n",
      "Copied: FlagDataType.F90 -> FlagDataType.txt\n",
      "Copied: EcoSimSumDataType.F90 -> EcoSimSumDataType.txt\n",
      "Copied: CanopyDataType.F90 -> CanopyDataType.txt\n",
      "Processing dir:./EcoSIM_code/f90src/Modelpars/\n",
      "Copied: NitroPars.F90 -> NitroPars.txt\n",
      "Copied: MicBGCPars.F90 -> MicBGCPars.txt\n",
      "Excluded: TracerPropMod.F90\n",
      "Copied: SoluteParMod.F90 -> SoluteParMod.txt\n",
      "Excluded: EcoSiMParDataMod.F90\n",
      "Copied: GrosubPars.F90 -> GrosubPars.txt\n",
      "Copied: ChemTracerParsMod.F90 -> ChemTracerParsMod.txt\n",
      "Processing dir:./EcoSIM_code/f90src/Utils/\n",
      "Copied: EcoSimConst.F90 -> EcoSimConst.txt\n"
     ]
    }
   ],
   "source": [
    "#download the github repo\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def git_clone(repository_url, destination_directory=None):\n",
    "    command = ['git', 'clone', repository_url]\n",
    "    if destination_directory:\n",
    "        command.append(destination_directory)\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"Repository cloned successfully.\")\n",
    "    else:\n",
    "        print(f\"Error cloning repository: {result.stderr}\")\n",
    "\n",
    "# Example usage:\n",
    "git_clone(\"git@github.com:jinyun1tang/EcoSIM.git\", \"EcoSIM_code\")\n",
    "\n",
    "\n",
    "#Copy .F90 files into .txt files\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def copy_f90_to_txt(source_dir, dest_dir, exclude_files=None,include_files=None):\n",
    "    \"\"\"Copy .F90 files from source_dir to dest_dir as .txt files, excluding specified files.\"\"\"\n",
    "    \n",
    "    source_path = Path(source_dir)\n",
    "    dest_path = Path(dest_dir)\n",
    "    exclude_files = exclude_files or []\n",
    "    include_files = include_files or []\n",
    "    print(f\"Processing dir:{source_dir}\")\n",
    "    # Create destination directory\n",
    "    dest_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Process all .F90 files (case insensitive)\n",
    "    if not include_files:\n",
    "        for pattern in [\"*.F90\", \"*.f90\"]:\n",
    "            for f90_file in source_path.glob(pattern):\n",
    "                if f90_file.stem not in exclude_files:\n",
    "                    dest_file = dest_path / (f90_file.stem + \".txt\")\n",
    "                    shutil.copy2(f90_file, dest_file)\n",
    "                    print(f\"Copied: {f90_file.name} -> {dest_file.name}\")\n",
    "                else:\n",
    "                    print(f\"Excluded: {f90_file.name}\")\n",
    "    else:\n",
    "        for pattern in [\"*.F90\", \"*.f90\"]:\n",
    "            for f90_file in source_path.glob(pattern):\n",
    "                if f90_file.stem in include_files:\n",
    "                    dest_file = dest_path / (f90_file.stem + \".txt\")\n",
    "                    shutil.copy2(f90_file, dest_file)\n",
    "                    print(f\"Copied: {f90_file.name} -> {dest_file.name}\")        \n",
    "\n",
    "# Usage example\n",
    "\n",
    "copy_f90_to_txt(\n",
    "        source_dir=\"./EcoSIM_code/f90src/Ecosim_datatype/\",\n",
    "        dest_dir=\"./txt/\", \n",
    "        exclude_files=[\"BalanceCheckDataType\",\"EcoSIMCtrlDataType\",\"EcoSIMHistMod\"]\n",
    "    )\n",
    "\n",
    "copy_f90_to_txt(\n",
    "        source_dir=\"./EcoSIM_code/f90src/Modelpars/\",\n",
    "        dest_dir=\"./txt/\", \n",
    "        exclude_files=[\"EcoSiMParDataMod\",\"TracerPropMod\"]\n",
    "    )\n",
    "\n",
    "copy_f90_to_txt(\n",
    "        source_dir=\"./EcoSIM_code/f90src/Utils/\",\n",
    "        dest_dir=\"./txt/\", \n",
    "        include_files=[\"EcoSimConst\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9fe59fd-24aa-4eee-bd74-cdf5ea65bd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EcosimBGCFluxType.txt', 'SoluteParMod.txt', 'GrosubPars.txt', 'FlagDataType.txt', 'ChemTracerParsMod.txt', 'EcoSimSumDataType.txt', 'PlantDataRateType.txt', 'IrrigationDataType.txt', 'CanopyDataType.txt', 'LandSurfDataType.txt', 'PlantTraitDataType.txt', 'ChemTranspDataType.txt', 'SoilBGCDataType.txt', 'GridDataType.txt', 'MicrobialDataType.txt', 'PlantMgmtDataType.txt', 'RootDataType.txt', 'SurfLitterDataType.txt', 'NitroPars.txt', 'SedimentDataType.txt', 'EcoSimConst.txt', 'ClimForcDataType.txt', 'SurfSoilDataType.txt', 'MicBGCPars.txt', 'SoilHeatDataType.txt', 'SoilPhysDataType.txt', 'SoilPropertyDataType.txt', 'SnowDataType.txt', 'FertilizerDataType.txt', 'CanopyRadDataType.txt', 'SOMDataType.txt', 'AqueChemDatatype.txt', 'SoilWaterDataType.txt']\n"
     ]
    }
   ],
   "source": [
    "# get name of all files need to be parsed.\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Method 1: One-liner with os.listdir\n",
    "directory = \"./txt\"\n",
    "txt_files = [f for f in os.listdir(directory) if f.lower().endswith('.txt')]\n",
    "\n",
    "# Method 2: One-liner with pathlib\n",
    "txt_files = [f.name for f in Path(directory).glob(\"*.txt\")]\n",
    "\n",
    "# Method 3: One-liner with glob\n",
    "import glob\n",
    "txt_files = [os.path.basename(f) for f in glob.glob(os.path.join(directory, \"*.txt\"))]\n",
    "\n",
    "print(txt_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a55a93b-d84a-4df9-96f5-0aaf3df2b907",
   "metadata": {},
   "source": [
    "# example data to extract patterns\n",
    "  real(r8),target,allocatable ::  canopy_growth_pft(:,:,:)                   !canopy structural growth rate [gC/h]\n",
    "  \n",
    "  real(r8) :: RMAX       !maximum hourly radiation,\t[MJ m-2 h-1]\n",
    "  \n",
    "  integer,target,allocatable ::  iPlantGrainType_pft(:,:,:)                  !grain type (below or above-ground), e.g. potato and onion are below,\n",
    "  \n",
    "  real(r8), PARAMETER :: DPH2O=6.5E-09_r8                !equilbrium constant for H2O=H(+)+OH(-), [mol^2 m^-6]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d23a165f-5989-41a4-919f-a16b1efee293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file [1]: EcosimBGCFluxType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 39. Stopping parsing.\n",
      "Parsing file [2]: SoluteParMod.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 138. Stopping parsing.\n",
      "Parsing file [3]: GrosubPars.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 99. Stopping parsing.\n",
      "Parsing file [4]: FlagDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 32. Stopping parsing.\n",
      "Parsing file [5]: ChemTracerParsMod.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 56. Stopping parsing.\n",
      "Parsing file [6]: EcoSimSumDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 45. Stopping parsing.\n",
      "Parsing file [7]: PlantDataRateType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 132. Stopping parsing.\n",
      "Parsing file [8]: IrrigationDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 84. Stopping parsing.\n",
      "Parsing file [9]: CanopyDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 169. Stopping parsing.\n",
      "Parsing file [10]: LandSurfDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 30. Stopping parsing.\n",
      "Parsing file [11]: PlantTraitDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 169. Stopping parsing.\n",
      "Parsing file [12]: ChemTranspDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 38. Stopping parsing.\n",
      "Parsing file [13]: SoilBGCDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 132. Stopping parsing.\n",
      "Parsing file [14]: GridDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 34. Stopping parsing.\n",
      "Parsing file [15]: MicrobialDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 59. Stopping parsing.\n",
      "Parsing file [16]: PlantMgmtDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 35. Stopping parsing.\n",
      "Parsing file [17]: RootDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 94. Stopping parsing.\n",
      "Parsing file [18]: SurfLitterDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 34. Stopping parsing.\n",
      "Parsing file [19]: NitroPars.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 109. Stopping parsing.\n",
      "Parsing file [20]: SedimentDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 40. Stopping parsing.\n",
      "Parsing file [21]: EcoSimConst.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 47. Stopping parsing.\n",
      "Parsing file [22]: ClimForcDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 133. Stopping parsing.\n",
      "Parsing file [23]: SurfSoilDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 32. Stopping parsing.\n",
      "Parsing file [24]: MicBGCPars.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 32. Stopping parsing.\n",
      "Parsing file [25]: SoilHeatDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 36. Stopping parsing.\n",
      "Parsing file [26]: SoilPhysDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 37. Stopping parsing.\n",
      "Parsing file [27]: SoilPropertyDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 40. Stopping parsing.\n",
      "Parsing file [28]: SnowDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 85. Stopping parsing.\n",
      "Parsing file [29]: FertilizerDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 25. Stopping parsing.\n",
      "Parsing file [30]: CanopyRadDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 25. Stopping parsing.\n",
      "Parsing file [31]: SOMDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 49. Stopping parsing.\n",
      "Parsing file [32]: AqueChemDatatype.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 84. Stopping parsing.\n",
      "Parsing file [33]: SoilWaterDataType.txt\n",
      "--------------------------------------------------\n",
      "  Encountered 'contains' at line 109. Stopping parsing.\n",
      "\n",
      "All variables saved to all_variables.csv\n",
      "Total variables: 1832\n",
      "Files processed: 33\n",
      "['Eco_NetRad_col', 'Eco_Heat_Latent_col', 'Eco_Heat_Sens_col', 'Eco_Heat_GrndSurf_col', 'Eco_GPP_CumYr_col']\n"
     ]
    }
   ],
   "source": [
    "# parse variables int varname, description\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_variable_info_with_termination(line):\n",
    "    \"\"\"\n",
    "    Extract variable name, description, and unit from Fortran variable declaration lines.\n",
    "    Returns 'TERMINATE' if line contains 'contains'.\n",
    "    \n",
    "    Args:\n",
    "        line (str): Line containing variable declaration\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (variable_name, description, unit) or None if no match, or 'TERMINATE' if contains found\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if line contains 'contains' (case insensitive)\n",
    "    found_strings = [s for s in ['contains','terminate'] if s in line.lower()]\n",
    "    if found_strings:\n",
    "        return 'TERMINATE'\n",
    "    \n",
    "    # Remove leading/trailing whitespace\n",
    "    line = line.strip()\n",
    "    \n",
    "    # Enhanced pattern to match various Fortran variable declarations\n",
    "    # Pattern to match variable declarations\n",
    "    pattern = r'^\\s*(real(?:\\([^)]*\\))?|integer)(?:\\s*,\\s*[^:]*?)?\\s*::\\s*([^!\\s=\\(]+)(?:\\([^)]*\\))?(?:\\s*=\\s*[^!]*)?\\s*!(.*)$'\n",
    "    \n",
    "    match = re.match(pattern, line.strip(), re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        variable_name = match.group(2).strip()\n",
    "        comment_part = match.group(3).strip()\n",
    "        \n",
    "        # Extract unit from square brackets\n",
    "        unit_match = re.search(r'\\[([^\\]]*)\\]', comment_part)\n",
    "        unit = unit_match.group(1).strip() if unit_match else \"\"\n",
    "        \n",
    "        # Extract description\n",
    "        if unit_match:\n",
    "            description = comment_part[:unit_match.start()].strip()\n",
    "        else:\n",
    "            description = comment_part.strip()\n",
    "        \n",
    "        # Clean description\n",
    "        description = re.sub(r'[,\\s]+$', '', description).strip()\n",
    "        description = re.sub(r'\\s+', ' ', description)\n",
    "        \n",
    "        return variable_name, description, unit\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_variables_simple_with_termination(file_path):\n",
    "    \"\"\"\n",
    "    Extract variables from a file, stopping when 'contains' is encountered.\n",
    "    \"\"\"\n",
    "    \n",
    "    variables = []\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            for line_num, line in enumerate(file, 1):\n",
    "                result = extract_variable_info_with_termination(line)\n",
    "                \n",
    "                if result == 'TERMINATE':\n",
    "                    print(f\"  Encountered 'contains' at line {line_num}. Stopping parsing.\")\n",
    "                    break\n",
    "                elif result:\n",
    "                    variables.append(result)\n",
    "#                    print(f\"  {result[0]}, {result[1]}, {result[2]}\")\n",
    "        \n",
    "        return variables\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error processing file {file_path}: {e}\")\n",
    "        return []\n",
    "# Usage\n",
    "# Initialize list to collect all variables\n",
    "all_variables = []\n",
    "k=0\n",
    "for file_path in txt_files:\n",
    "    k=k+1\n",
    "    print('Parsing file [%d]: %s'%(k,file_path))\n",
    "    print('-'*50)\n",
    "    variables = extract_variables_simple_with_termination('./txt/'+file_path)\n",
    "    for var_name, description, unit in variables:\n",
    "        all_variables.append({\n",
    "            'File Name':file_path,\n",
    "            'Variable Name': var_name,\n",
    "            'Description': description,\n",
    "            'Unit': unit\n",
    "        })\n",
    "# Write all variables to CSV file\n",
    "if all_variables:\n",
    "    df = pd.DataFrame(all_variables)\n",
    "    df.to_csv('new_EcoSIM_variables.csv', index=False, encoding='utf-8')\n",
    "    print(f'\\nAll variables saved to all_variables.csv')\n",
    "    print(f'Total variables: {len(all_variables)}')\n",
    "    print(f'Files processed: {k}')\n",
    "else:\n",
    "    print('No variables found to save.')    \n",
    "\n",
    "# Extract all Variable Name entries\n",
    "ecosim_new_variables = [item['Variable Name'] for item in all_variables]\n",
    "ecosim_new_descriptions=[item['Description'] for item in all_variables]\n",
    "print(ecosim_new_variables[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d947e139-6685-43b6-a6f6-eb40b84990ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dictionary with 1272 variables\n",
      "Names vector length: 1274\n",
      "Descriptions vector length: 1274\n",
      "Names vector length: 1180\n",
      "Descriptions vector length: 1180\n"
     ]
    }
   ],
   "source": [
    "#read EcoSIM.xlsx that holds variables from the old version.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_ecosim_dict(file_path=\"EcoSIM.xlsx\"):\n",
    "    \"\"\"\n",
    "    Function to create EcoSIM dictionary and return two vectors of strings.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (ecosim_dict, names_vector, descriptions_vector)\n",
    "               - ecosim_dict: Dictionary mapping names to descriptions\n",
    "               - names_vector: List of EcoSIM Other Names\n",
    "               - descriptions_vector: List of corresponding descriptions\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Read Excel file, skip second row\n",
    "        df = pd.read_excel(file_path, skiprows=[1])\n",
    "        \n",
    "        # Extract required columns and remove NaN variable names\n",
    "        subset_df = df[['EcoSIM Other Names', 'Description']].dropna(subset=['EcoSIM Other Names'])\n",
    "        \n",
    "        # Create vectors of strings\n",
    "        nm_vector = subset_df['EcoSIM Other Names'].astype(str).tolist()\n",
    "        names_vector= [var.replace('ECOSIM:', '') for var in nm_vector]\n",
    "\n",
    "        descriptions_strings = subset_df['Description'].fillna('').astype(str).tolist()\n",
    "        keywords=['refers', 'presents', 'represents',' is ','denotes','stands for','indicates','this ', 'are defined']\n",
    "        descriptions_vector =[s[:min([s.lower().find(k.lower()) for k in keywords if k.lower() in s.lower()] or [len(s)])].strip() for s in descriptions_strings]\n",
    "\n",
    "        \n",
    "        # Create dictionary\n",
    "        ecosim_dict = dict(zip(names_vector, descriptions_vector))\n",
    "        \n",
    "        print(f\"Created dictionary with {len(ecosim_dict)} variables\")\n",
    "        print(f\"Names vector length: {len(names_vector)}\")\n",
    "        print(f\"Descriptions vector length: {len(descriptions_vector)}\")\n",
    "        \n",
    "        return ecosim_dict, names_vector, descriptions_vector\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None, [], []\n",
    "# Usage:\n",
    "ecosim_dict, ecosim_old_names_vector, ecosim_old_descriptions_vector = create_ecosim_dict(\"EcoSIM.xlsx\")\n",
    "\n",
    "\n",
    "def clean_paired_vectors(vec_1, vec_2):\n",
    "    \"\"\"\n",
    "    Simple and efficient function to clean paired vectors.\n",
    "    \n",
    "    Args:\n",
    "        vec_1, vec_2 (list): Input vectors of same length\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (cleaned_vec_1, cleaned_vec_2)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Common units to remove\n",
    "    units_to_remove = {\n",
    "        'oC', 'kg', 'uM', 'm2', 'g', 'mg', 'cm', 'mm', 'km', 'L', 'mL',\n",
    "        'Pa', 'kPa', 'MPa', 'mol', 'mmol', 'umol', 'ppmv', 'ppb', 'pH',\n",
    "        'm', 's', 'h', 'd', 'yr', 'C', 'K', 'F', 'J', 'kJ', 'MJ', 'W',\n",
    "        'kW', 'V', 'A', 'Hz', 'bar', 'atm', '%', 'pct','m3'\n",
    "    }\n",
    "    var_numeric_id=range(len(vec_1))\n",
    "    \n",
    "    cleaned_1, cleaned_2, cleaned_3 = [], [],[]\n",
    "    \n",
    "    for i in range(len(vec_1)):\n",
    "        entry = str(vec_1[i]).strip()\n",
    "        \n",
    "        # Keep entry if it doesn't match removal criteria\n",
    "        if (len(entry) > 1 and \n",
    "            '-' not in entry and \n",
    "            entry not in units_to_remove) and 'CATEGORY' not in entry:\n",
    "            cleaned_1.append(var_numeric_id[i])\n",
    "            cleaned_2.append(vec_1[i])\n",
    "            cleaned_3.append(vec_2[i])\n",
    "    \n",
    "    return cleaned_1, cleaned_2, cleaned_3\n",
    "\n",
    "#remove units\n",
    "ecosim_old_varid_vector_clean,ecosim_old_names_vector_clean, ecosim_old_descriptions_vector_clean=clean_paired_vectors(ecosim_old_names_vector, ecosim_old_descriptions_vector)\n",
    "       \n",
    "print(f\"Names vector length: {len(ecosim_old_names_vector_clean)}\")\n",
    "print(f\"Descriptions vector length: {len(ecosim_old_descriptions_vector_clean)}\")\n",
    "\n",
    "\n",
    "# Create DataFrame and write to CSV\n",
    "df = pd.DataFrame({\n",
    "    'Variable_ID': ecosim_old_varid_vector_clean,\n",
    "    'Variable_Name': ecosim_old_names_vector_clean,\n",
    "    'Description': ecosim_old_descriptions_vector_clean\n",
    "})\n",
    "\n",
    "# Write to CSV\n",
    "df.to_csv('ecosim_old_clean_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76db3a6b-8e7c-4e45-a91b-2b90cdd3bcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:14: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.0)\n",
      "  from scipy.sparse import csr_matrix, issparse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding semantic matches...\n",
      "Loading sentence transformer model...\n",
      "Generating embeddings...\n",
      "\n",
      "Semantic Similarity Matches:\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#match the variable names by semantic meaning\n",
    "# First install required packages:\n",
    "# pip install sentence-transformers scikit-learn numpy\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "\n",
    "def match_strings_semantic_similarity(vector1, vector2, top_k=3, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Match strings between two vectors using semantic similarity with Sentence Transformers.\n",
    "    \n",
    "    Args:\n",
    "        vector1, vector2 (list): Lists of strings to match\n",
    "        top_k (int): Number of top matches to return for each string\n",
    "        threshold (float): Minimum similarity threshold (0-1)\n",
    "        \n",
    "    Returns:\n",
    "        list: List of match results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load pre-trained sentence transformer model\n",
    "    print(\"Loading sentence transformer model...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast and good quality\n",
    "    \n",
    "    # Generate embeddings for both vectors\n",
    "    print(\"Generating embeddings...\")\n",
    "    embeddings1 = model.encode(vector1)\n",
    "    embeddings2 = model.encode(vector2)\n",
    "    \n",
    "    # Calculate cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(embeddings1, embeddings2)\n",
    "    \n",
    "    # Find matches for each string in vector1\n",
    "    matches = []\n",
    "    \n",
    "    for i, string1 in enumerate(vector1):\n",
    "        # Get similarities for current string\n",
    "        similarities = similarity_matrix[i]\n",
    "        \n",
    "        # Get top k matches above threshold\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        top_matches = []\n",
    "        \n",
    "        for idx in top_indices:\n",
    "            if similarities[idx] >= threshold:\n",
    "                top_matches.append({\n",
    "                    'string': vector2[idx],\n",
    "                    'similarity': similarities[idx],\n",
    "                    'index': idx\n",
    "                })\n",
    "        \n",
    "        matches.append({\n",
    "            'source_string': string1,\n",
    "            'source_index': i,\n",
    "            'matches': top_matches\n",
    "        })\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Test with your example\n",
    "#vector1 = [\n",
    "#    'Snow temperature (TCSnow)',\n",
    "#    'The threshold temperature for autumn leafoff/hardening',\n",
    "#    'The threshold temperature for spring leafout/dehardening, often abbreviated as TCZ'\n",
    "#]\n",
    "\n",
    "#vector2 = [\n",
    "#    'Temperature measurement in snow layers',\n",
    "#    'Critical temperature for plant dormancy in fall',\n",
    "#    'Spring awakening temperature for vegetation',\n",
    "#    'Soil moisture content measurement',\n",
    "#    'Canopy leaf area calculation',\n",
    "#    'Root biomass estimation method'\n",
    "#]\n",
    "\n",
    "# Find semantic matches\n",
    "print(\"Finding semantic matches...\")\n",
    "matches = match_strings_semantic_similarity(ecosim_old_descriptions_vector_clean, ecosim_new_descriptions, top_k=2, threshold=0.2)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nSemantic Similarity Matches:\")\n",
    "print(\"=\" * 80)\n",
    "ecosim_old_varid,ecosim_old_varname,ecosim_old_vardescp,ecomsim_1stbest_newvar_match,ecomsim_2ndbest_newvar_match=[],[],[],[],[]\n",
    "for match in matches:\n",
    "    ecosim_old_varid.append(match['source_index'])\n",
    "    ecosim_old_varname.append(ecosim_old_names_vector_clean[match['source_index']])\n",
    "    ecosim_old_vardescp.append(match['source_string'])\n",
    "    \n",
    "    if match['matches']:\n",
    "        match0=(match['matches'])        \n",
    "        ecomsim_1stbest_newvar_match.append(ecosim_new_variables[match0[0]['index']])\n",
    "        ecomsim_2ndbest_newvar_match.append(ecosim_new_variables[match0[1]['index']])        \n",
    "    else:\n",
    "        ecomsim_1stbest_newvar_match.append('')\n",
    "        ecomsim_2ndbest_newvar_match.append('')\n",
    "\n",
    "# Create DataFrame and write to CSV\n",
    "df = pd.DataFrame({\n",
    "    'Old_Variable_ID': ecosim_old_varid,\n",
    "    'Old_Variable_Name': ecosim_old_varname,\n",
    "    'Description': ecosim_old_vardescp,\n",
    "    'Best macth new var': ecomsim_1stbest_newvar_match,\n",
    "    'Next best match new var':ecomsim_1stbest_newvar_match\n",
    "})\n",
    "\n",
    "# Write to CSV\n",
    "df.to_csv('ecosim_old_new_varmatch.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f7c2d0-d6a2-4bd6-a3dc-30dbe2c52c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#match the old and new variables\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "#variable matching use cosine similarity\n",
    "def find_most_similar_strings(target_string, candidate_strings, top_k=5):\n",
    "    \"\"\"\n",
    "    Find the most similar strings to a target string.\n",
    "    \n",
    "    Args:\n",
    "        target_string (str): String to compare against\n",
    "        candidate_strings (list): List of candidate strings\n",
    "        top_k (int): Number of top matches to return\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tuples (string, similarity_score)\n",
    "    \"\"\"\n",
    "    \n",
    "    def count_chars_to_vector(input_string):\n",
    "        count_vector = [0] * 36\n",
    "        for char in input_string.lower():\n",
    "            if char.isdigit():\n",
    "                count_vector[int(char)] += 1\n",
    "            elif char.isalpha():\n",
    "                count_vector[ord(char) - ord('a') + 10] += 1\n",
    "        return count_vector\n",
    "    \n",
    "    def cosine_similarity(vec1, vec2):\n",
    "        dot_product = sum(a * b for a, b in zip(vec1, vec2))\n",
    "        magnitude1 = math.sqrt(sum(a * a for a in vec1))\n",
    "        magnitude2 = math.sqrt(sum(b * b for b in vec2))\n",
    "        \n",
    "        if magnitude1 > 0 and magnitude2 > 0:\n",
    "            return dot_product / (magnitude1 * magnitude2)\n",
    "        return 0\n",
    "    \n",
    "    target_vector = count_chars_to_vector(target_string)\n",
    "    similarities = []\n",
    "    \n",
    "    for candidate in candidate_strings:\n",
    "        candidate_vector = count_chars_to_vector(candidate)\n",
    "        similarity = cosine_similarity(target_vector, candidate_vector)\n",
    "        similarities.append((candidate, similarity))\n",
    "    \n",
    "    # Sort by similarity (descending) and return top k\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    matches = \" | \".join([f\"{m[0]} ({m[1]:.3f})\" for m in similarities[:top_k]])\n",
    "    return similarities[:top_k]\n",
    "\n",
    "# Test with EcoSIM variable names\n",
    "ecosim_simmatch_1stbest_var,ecosim_simmatch_2ndbest_var=[],[]\n",
    "for target in ecosim_old_varname:\n",
    "    matches = find_most_similar_strings(target, ecosim_new_variables, top_k=3)\n",
    "    all_strings = [item[0] for item in matches]\n",
    "    ecosim_simmatch_1stbest_var.append(all_strings[0])\n",
    "    ecosim_simmatch_2ndbest_var.append(all_strings[1])\n",
    "    \n",
    "    \n",
    "df = pd.DataFrame({\n",
    "    'Old_Variable_ID': ecosim_old_varid,\n",
    "    'Old_Variable_Name': ecosim_old_varname,\n",
    "    'Description': ecosim_old_vardescp,\n",
    "    'Best semantic macth new var': ecomsim_1stbest_newvar_match,\n",
    "    'Next best semantic match new var':ecomsim_1stbest_newvar_match,\n",
    "    'Best similarity match new vaar':ecosim_simmatch_1stbest_var,\n",
    "    'Next best similarity match new vaar':ecosim_simmatch_2ndbest_var    \n",
    "})\n",
    "\n",
    "# Write to CSV, which has the variables aligned by cosine similarity and semantic similarity\n",
    "df.to_csv('ecosim_old_new_var_fullmatch.csv', index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a35be-b5a3-430d-ae72-30468cb8f831",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
